from typing import Optional, Tuple, Union
from dataclasses import dataclass

import torch
from transformers import T5ForConditionalGeneration, T5Config, AutoTokenizer
from transformers.modeling_outputs import Seq2SeqLMOutput, BaseModelOutput
from transformers.utils import ModelOutput, logging #
import warnings #
import os
import torch.nn as nn # 确保导入 nn
# import argparse # argparse is not needed if hardcoding paths

# 获取 HuggingFace logger
logger = logging.get_logger(__name__)


@dataclass
class UnifiedPAGOutput(ModelOutput):
    """
    Output class for the UnifiedPAGModel.
    - bow_logits: Logits for the Bag-of-Words prediction part.
                 Shape: [batch_size, encoder_input_length, vocab_size].
                 The length `encoder_input_length` is dynamic.
    - seq_id_logits: Logits for the Sequential ID (Voken) prediction part.
                   Shape: [batch_size, num_seq_id_tokens, vocab_size].
                   The length `num_seq_id_tokens` is fixed.
    Other fields are standard T5 outputs. Loss is computed externally.
    """
    loss: Optional[torch.FloatTensor] = None # Will be None, computed in Trainer
    bow_logits: Optional[torch.FloatTensor] = None
    seq_id_logits: Optional[torch.FloatTensor] = None
    # For compatibility and generation
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None
    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None
    encoder_last_hidden_state: Optional[torch.FloatTensor] = None
    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None


class UnifiedPAGModel(T5ForConditionalGeneration):
    def __init__(self, config: T5Config, num_seq_id_tokens: int = 4):
        super().__init__(config)
        # num_seq_id_tokens defines the fixed length of the Voken sequence part
        # generated by the decoder after the BoW part.
        self.num_seq_id_tokens = num_seq_id_tokens
        # The BoW part's length will be dynamically determined by encoder_sequence_length.
        # ---- 确保打分头存在 ----
        if not hasattr(config, 'pag_code_book_size'):

            print("Warning: config missing 'pag_code_book_size'. Assuming 1024.")
            config.pag_code_book_size = 1024 # 示例
        self.pag_code_book_size = config.pag_code_book_size
        self.seq_id_preference_head = nn.Linear(config.d_model, self.pag_code_book_size)
        # ---- 结束确保打分头存在 ----

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        decoder_input_ids: Optional[torch.LongTensor] = None,
        decoder_attention_mask: Optional[torch.BoolTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        decoder_head_mask: Optional[torch.FloatTensor] = None,
        cross_attn_head_mask: Optional[torch.Tensor] = None,
        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,
        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple[torch.FloatTensor], UnifiedPAGOutput]:

        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if head_mask is not None and decoder_head_mask is None:
            if self.config.num_layers == self.config.num_decoder_layers:
                warnings.warn("""
The input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,
`decoder_head_mask` is set to `head_mask_provided_by_the_user`, if `head_mask` is provided, otherwise it is set to None.
This behavior is deprecated and will be removed in v5.0.0. Please use `decoder_head_mask` to specify the mask for the decoder.
""")
                decoder_head_mask = head_mask

        # Encode
        if encoder_outputs is None:
            if input_ids is None and inputs_embeds is None:
                raise ValueError("Either `input_ids` or `inputs_embeds` must be provided if `encoder_outputs` is not.")
            encoder_outputs = self.encoder(
                input_ids=input_ids,
                attention_mask=attention_mask,
                inputs_embeds=inputs_embeds,
                head_mask=head_mask,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
            )
        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):
            encoder_outputs = BaseModelOutput(
                last_hidden_state=encoder_outputs[0],
                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,
                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,
            )

        hidden_states = encoder_outputs[0] # Encoder last hidden state

        # Determine encoder_sequence_length, crucial for splitting decoder output for BoW
        if input_ids is not None:
            encoder_sequence_length = input_ids.size(1)
        elif inputs_embeds is not None:
            encoder_sequence_length = inputs_embeds.size(1)
        elif encoder_outputs.last_hidden_state is not None:
            encoder_sequence_length = encoder_outputs.last_hidden_state.size(1)
        else:
            # This should not happen in a standard training/inference setup where query is provided.
            raise ValueError(
                "Cannot determine `encoder_sequence_length`. "
                "Provide `input_ids`, `inputs_embeds`, or `encoder_outputs` with `last_hidden_state`."
            )
            
        if self.model_parallel:
            torch.cuda.set_device(self.decoder.first_device)

        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:
            # labels are expected to be of shape [batch_size, encoder_sequence_length + num_seq_id_tokens]
            decoder_input_ids = self._shift_right(labels)

        # Device settings for model parallelism
        if self.model_parallel:
            torch.cuda.set_device(self.decoder.first_device)
            hidden_states = hidden_states.to(self.decoder.first_device)
            if decoder_input_ids is not None:
                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)
            if attention_mask is not None:
                attention_mask = attention_mask.to(self.decoder.first_device)
            if decoder_attention_mask is not None:
                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)

        # Decode
        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            inputs_embeds=decoder_inputs_embeds,
            past_key_values=past_key_values,
            encoder_hidden_states=hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=decoder_head_mask,
            cross_attn_head_mask=cross_attn_head_mask,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        sequence_output = decoder_outputs[0] # Shape: [batch_size, current_decoder_seq_len, d_model]

        if self.config.tie_word_embeddings: # T5-specific
            sequence_output = sequence_output * (self.model_dim**-0.5)

        current_decoder_seq_len = sequence_output.size(1)
        
        # Part 1: BoW Logits
        # The length of this part of decoder output is determined by encoder_sequence_length.
        # The BoW loss function will need to handle these variable-length logits.
        # (e.g., by predicting a distribution for each input query token position,
        # or by pooling/selecting specific positions if actual BoW target is fixed length M).
        len_for_bow_output = min(current_decoder_seq_len, encoder_sequence_length)
        bow_sequence_part = sequence_output[:, :len_for_bow_output, :]
        bow_logits = self.lm_head(bow_sequence_part) # Shape: [batch_size, len_for_bow_output, vocab_size]

        # Part 2: Seq-ID Logits (Vokens)
        # This part follows the BoW part and has a fixed length of self.num_seq_id_tokens.
        seq_id_logits = None
        start_idx_for_seq_id = encoder_sequence_length # BoW part occupies up to this length
        
        if current_decoder_seq_len > start_idx_for_seq_id:
            # Ensure we don't try to slice beyond the actual generated length for Seq-ID part
            len_for_seq_id_output = min(self.num_seq_id_tokens, current_decoder_seq_len - start_idx_for_seq_id)
            
            if len_for_seq_id_output > 0:
                seq_id_sequence_part = sequence_output[:, start_idx_for_seq_id : start_idx_for_seq_id + len_for_seq_id_output, :]
                seq_id_logits = self.lm_head(seq_id_sequence_part) # Shape: [batch_size, len_for_seq_id_output, vocab_size]
        
        # Loss is not computed in this model class
        loss = None

        if not return_dict:
            logger.warning(
                "Non-dict output for UnifiedPAGModel is not fully implemented and may be inconsistent. "
                "Please use return_dict=True."
            )
            # Simplified non-dict output for now, focusing on primary logits
            encoder_last_hidden_state_tuple_part = (encoder_outputs.last_hidden_state,) if return_dict else (encoder_outputs[0],)
            
            # Collect all decoder outputs except the first (sequence_output already processed)
            processed_decoder_outputs = ()
            if return_dict :
                if decoder_outputs.past_key_values is not None:
                    processed_decoder_outputs += (decoder_outputs.past_key_values,)
                if decoder_outputs.hidden_states is not None:
                    processed_decoder_outputs += (decoder_outputs.hidden_states,)
                if decoder_outputs.attentions is not None:
                    processed_decoder_outputs += (decoder_outputs.attentions,)
                if decoder_outputs.cross_attentions is not None:
                    processed_decoder_outputs += (decoder_outputs.cross_attentions,)
            else: # decoder_outputs is a tuple
                processed_decoder_outputs += decoder_outputs[1:]

            output_tuple = (bow_logits, seq_id_logits) + processed_decoder_outputs + encoder_last_hidden_state_tuple_part
            
            # Append other encoder outputs if they exist
            _encoder_hidden_states = encoder_outputs.hidden_states if return_dict else (encoder_outputs[1] if len(encoder_outputs) > 1 else None)
            _encoder_attentions = encoder_outputs.attentions if return_dict else (encoder_outputs[2] if len(encoder_outputs) > 2 else None)
            if _encoder_hidden_states is not None:
                output_tuple += (_encoder_hidden_states,)
            if _encoder_attentions is not None:
                output_tuple += (_encoder_attentions,)

            return ((loss,) + output_tuple) if loss is not None else output_tuple

        return UnifiedPAGOutput(
            loss=loss, # Will be None
            bow_logits=bow_logits,
            seq_id_logits=seq_id_logits,
            past_key_values=decoder_outputs.past_key_values,
            decoder_hidden_states=decoder_outputs.hidden_states,
            decoder_attentions=decoder_outputs.attentions,
            cross_attentions=decoder_outputs.cross_attentions,
            encoder_last_hidden_state=encoder_outputs.last_hidden_state,
            encoder_hidden_states=encoder_outputs.hidden_states,
            encoder_attentions=encoder_outputs.attentions,
        )

# def get_args(): # Not needed if hardcoding
#     pass

if __name__ == "__main__":
    # ---- 1. 定义路径 ----
    model_as_base_dir = "/home/iiserver33/Workbench/likai/avg/output/retriever/coco/t5-base/20250507_0929_coco_VT_1024-512_1-c1024_e3000_lr0.0001_mse_c1024_ep100_lr0.001_bch128_embadded/checkpoint-45084/" # 保留 Decoder/Embedding/LM_Head 的模型
    encoder_checkpoint_path = "/home/iiserver33/Workbench/likai/avg/tools/coco_stage1_output/encoder_step_3000.pt" # <--- 修改为你的 encoder 权重路径
    guidance_head_checkpoint_path = "/home/iiserver33/Workbench/likai/avg/tools/coco_stage1_output/seq_id_preference_head_step_3000.pt" # <--- 修改为你的打分头权重路径
    output_dir = "/home/iiserver33/Workbench/likai/avg/output/retriever/coco/merged_encoder_guidance_model_3000_beam10" # 新的输出路径

    # ---- 2. 加载基础模型和 Tokenizer ----
    print(f"Loading base model (keeping Decoder/Emb/LMHead) from: {model_as_base_dir}")
    # 使用基础模型的 Tokenizer
    tokenizer_final = AutoTokenizer.from_pretrained(model_as_base_dir)
    # 加载基础模型结构 (确保 UnifiedPAGModel 类被正确加载或定义)

    try:
        # 尝试加载为 UnifiedPAGModel，如果你的 base checkpoint 保存了这个结构
        model_merged = UnifiedPAGModel.from_pretrained(model_as_base_dir)
        print("Loaded base model as UnifiedPAGModel.")
        # 确保打分头存在
        if not hasattr(model_merged, 'seq_id_preference_head'):
             print("Base model lacks seq_id_preference_head. Adding it...")
             # 你需要知道 d_model 和 pag_code_book_size
             d_model = model_merged.config.d_model
             pag_code_book_size = 1024 # 或者从 config 获取
             model_merged.seq_id_preference_head = nn.Linear(d_model, pag_code_book_size).to(model_merged.device)

    except Exception as e:
        print(f"Could not load base as UnifiedPAGModel ({e}). Loading as T5ForConditionalGeneration and adding head manually.")
        model_merged = T5ForConditionalGeneration.from_pretrained(model_as_base_dir)
        # 手动添加打分头
        d_model = model_merged.config.d_model
        pag_code_book_size = 1024 #
        model_merged.seq_id_preference_head = nn.Linear(d_model, pag_code_book_size).to(model_merged.device)
        # 将其注册为模型的一部分（如果需要保存）
        model_merged.add_module("seq_id_preference_head", model_merged.seq_id_preference_head)


    # ---- 3. 加载 Encoder 权重 ----
    print(f"Loading encoder weights from: {encoder_checkpoint_path}")
    if os.path.exists(encoder_checkpoint_path):
        encoder_state_dict_full = torch.load(encoder_checkpoint_path, map_location='cpu')
        encoder_state_dict_filtered = {}
        for k, v in encoder_state_dict_full.items():
            if k.startswith("encoder.") and not k.startswith("encoder.embed_tokens"):
                encoder_state_dict_filtered[k.replace("encoder.", "", 1)] = v

        missing_keys, unexpected_keys = model_merged.encoder.load_state_dict(encoder_state_dict_filtered, strict=False)
        print(f"Encoder weights loaded. Missing keys: {missing_keys}, Unexpected keys: {unexpected_keys}")
        if unexpected_keys:
             print(f"Warning: Unexpected keys found when loading encoder state dict: {unexpected_keys}")
    else:
        print(f"Warning: Encoder checkpoint file not found: {encoder_checkpoint_path}. Encoder weights remain from base model.")

    # ---- 4. 加载打分头权重 ----
    print(f"Loading guidance head weights from: {guidance_head_checkpoint_path}")
    if os.path.exists(guidance_head_checkpoint_path):
        guidance_head_state_dict = torch.load(guidance_head_checkpoint_path, map_location='cpu')
        # 确保 merged_model 有打分头属性
        if hasattr(model_merged, 'seq_id_preference_head'):
             # 加载权重，注意可能需要调整键名（如果保存时带了前缀）
             # 假设 guidance_head_checkpoint_path 直接保存的是 Linear 层的 state_dict
             try:
                missing_keys_head, unexpected_keys_head = model_merged.seq_id_preference_head.load_state_dict(guidance_head_state_dict, strict=True) # 通常打分头结构简单，用 strict=True
                print(f"Guidance head weights loaded.")
             except RuntimeError as e:
                 print(f"Error loading guidance head state dict directly: {e}")
                 print("Attempting to load assuming potential prefix...")
                 # 如果加载失败，尝试移除可能的 'module.' 前缀（DDP 保存时可能有）
                 cleaned_state_dict = {k.replace("module.", ""): v for k, v in guidance_head_state_dict.items()}
                 try:
                    missing_keys_head, unexpected_keys_head = model_merged.seq_id_preference_head.load_state_dict(cleaned_state_dict, strict=True)
                    print(f"Guidance head weights loaded after cleaning keys.")
                 except Exception as e2:
                     print(f"Failed to load guidance head state dict even after cleaning keys: {e2}")

        else:
             print(f"Warning: merged_model does not have 'seq_id_preference_head'. Guidance head weights not loaded.")

    else:
        print(f"Warning: Guidance head checkpoint file not found: {guidance_head_checkpoint_path}. Guidance head weights remain initialized.")


    # ---- 5. 保存融合后的模型 ----
    print(f"Saving model with fused encoder and guidance head to: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    model_merged.save_pretrained(output_dir)
    tokenizer_final.save_pretrained(output_dir)

    print("Merging complete (encoder and guidance head only).")

    # ---- 6. 检查代码 (保持不变) ----
    # ... (你的检查代码) ...

    # Clean up
    del model_merged #, model_other, model_temp_base # 其他模型不再需要
    torch.cuda.empty_cache() # If using GPU


